
//::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
/** @author  John Miller
 *  @version 1.6
 *  @date    Mon Sep  9 13:30:41 EDT 2013
 *  @see     LICENSE (MIT style license file).
 *
 *  @title   Model: Perceptron (single output 2-layer Neural-Network)
 *
 *  @see     hebb.mit.edu/courses/9.641/2002/lectures/lecture03.pdf
 */

package scalation
package modeling

import scalation.mathstat._

import ActivationFun._
import Initializer._

//::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
/** The `Perceptron` class supports single-output, 2-layer (input and output)
 *  Neural-Networks.  Although perceptrons are typically used for classification,
 *  this class is used for prediction.  Given several input vectors and output
 *  values (training data), fit the weights/parameters 'b' connecting the layers,
 *  so that for a new input vector 'z', the net can predict the output value, i.e.,
 *      z = f (b dot z)
 *  The parameter vector 'b' (w) gives the weights between input and output layers.
 *  Note, 'b0' is treated as the bias, so 'x0' must be 1.0.
 *  @param x       the data/input m-by-n matrix (data consisting of m input vectors)
 *  @param y       the response/output m-vector (data consisting of m output values)
 *  @param fname_  the feature/variable names
 *  @param hparam  the hyper-parameters for the model/network
 *  @param f       the activation function family for layers 1->2 (input to output)
 *  @param itran   the inverse transformation function returns responses to original scale
 */
class Perceptron (x: MatrixD, y: VectorD, fname_ : Array [String] = null,
                  hparam: HyperParameter = Perceptron.hp,
                  f: AFF = f_sigmoid, val itran: FunctionV2V = null)
      extends Predictor (x, y, fname_, hparam)
         with Fit (dfm = x.dim2 - 1, df = x.dim - x.dim2):

    private val debug     = debugf ("Perceptron", false)            // debug function
    private val flaw      = flawf ("Perceptron")                    // flaw function
    private val (m, n)    = x.dims                                  // input data matrix dimensions
    private var eta       = hparam ("eta").toDouble                 // the learning/convergence rate (requires adjustment)
    private var bSize     = hparam ("bSize").toInt                  // the batch size
    private val maxEpochs = hparam ("maxEpochs").toInt              // the maximum number of training epcochs/iterations
    private val _1        = VectorD.one (m)                         // vector of all ones

    if y.dim != m then flaw ("constructor", "dimensions of x and y are incompatible")

    println (s"Create a Perceptron with $n input nodes and 1 output node")

    //::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
    /** Set the initial parameter/weight vector 'b' manually before training.
     *  This is mainly for testing purposes.
     *  @param w0  the initial weights for b
     */
    def setWeights (w0: VectorD): Unit = b = w0

    //::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
    /** Reset the learning rate 'eta'.
     *  @param eta  the learning rate
     */
    def reset (eta_ : Double): Unit = eta = eta_

    //::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
    /** Given training data 'x_' and 'y_', fit the parameter/weight vector 'b'.
     *  Minimize the error in the prediction by adjusting the weight vector 'b'.
     *  The error 'e' is simply the difference between the target value 'y_' and the
     *  predicted value 'yp'.  Minimize the dot product of error with itself using
     *  gradient-descent (move in the opposite direction of the gradient).
     *  Iterate over several epochs (no batching).
     *  Use val d  = yp * (_1 - yp) * e                             // delta y (for sigmoid only)
     *  @param x_  the training/full data/input matrix
     *  @param y_  the training/full response/output vector
     */
    def train (x_ : MatrixD = x, y_ : VectorD = y): Unit =
        println (s"train0: eta = $eta")
        if b == null then b = weightVec (n)                         // initialize parameters/weights
        var sse0 = Double.MaxValue

        for epoch <- 1 to maxEpochs do                              // epoch-th learning phase
            val yp = f.f_ (x_ * b)                                  // predicted output vector yp = f(Xb)
            e      = y_ - yp                                        // error vector for y
            val d  = -f.d (yp) * e                                  // delta vector for y
            b     -= x_.transpose * d * eta                         // update the parameters/weights

            val sse = (y_ - f.f_ (x_ * b)).normSq                   // recompute sum of squared errors
            debug ("train0", s"parameters for $epoch th epoch: b = $b, sse = $sse")
            if sse >= sse0 then return                              // return when sse increases
            sse0 = sse                                              // save prior sse
        end for
    end train

    //::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
    /** Test a predictive model y_ = f(x_) + e and return its QoF vector.
     *  Testing may be be in-sample (on the training set) or out-of-sample
     *  (on the testing set) as determined by the parameters passed in.
     *  Note: must call train before test.
     *  @param x_  the testing/full data/input matrix (defaults to full x)
     *  @param y_  the testing/full response/output vector (defaults to full y)
     */
    def test (x_ : MatrixD = x, y_ : VectorD = y): (VectorD, VectorD) =
        val yp = predict (x_)                                            // make predictions
        e = y_ - yp                                                      // RECORD the residuals/errors (@see `Predictor`)
        (yp, diagnose (y_, yp))                                          // return predictions and QoF vector
    end test

    //::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
    /** Given training data 'x_' and 'y_', fit the parameter/weight vector 'b'.
     *  Minimize the error in the prediction by adjusting the weight vector 'b'.
     *  Iterate over several epochs, where each epoch divides the training set into
     *  'nbat' batches.  Each batch is used to update the weights.
     *  @param x_  the training/full data/input matrix
     *  @param y_  the training/full response/output vector
    def train (x_ : MatrixD = x, y_ : VectorD = y): Unit =
        if y_.dim < 2 * bSize then flaw ("train", "not enough data for batching - use 'train0'")
        println (s"train: eta = $eta")
        if b == null then b = weightVec (n)                            // initialize parameters/weights
        val result = optimize (x_, y_, b, eta, bSize, maxEpochs, f)
        println (s"result = (sse, ending_epoch_) = $result")
    end train
     */

    //::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
    /** Given training data 'x_' and 'y_', fit the parameter/weight vector 'b'.
     *  Minimize the error in the prediction by adjusting the weight vector 'b'.
     *  Iterate over several epochs, where each epoch divides the training set into
     *  'nbat' batches.  Each batch is used to update the weights.
     *  This version preforms an interval search for the best 'eta' value.
     *  @param x_  the training/full data/input matrix
     *  @param y_  the training/full response/output vector
    override def train2 (x_ : MatrixD = x, y_ : VectorD = y): Unit =
        if y_.dim < 2 * bSize then flaw ("train2", "not enough data for batching - use 'train0'")
        val etaI = (0.25 * eta, 4.0 * eta)                          // quarter to four times eta
        println (s"train2: etaI = $etaI")
        if b == null then b = weightVec (n)                            // initialize parameters/weights
        val result = optimizeI (x_, y_, b, etaI, bSize, maxEpochs, f)
        println (s"result = (sse, ending_epoch_) = $result")
    end train2
     */

    //::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
    /** Switch between 'train' methods: simple (0), regular (1) and hyper-parameter
     *  optimizing (2).
     *  @param which  the kind of 'train' method to use
     *  @param x_  the training/full data/input matrix
     *  @param y_  the training/full response/output vector
    def trainSwitch (which: Int, x_ : MatrixD = x, y_ : VectorD = y): Perceptron =
        which match
        case 0 => train0 (x_, y_)
        case 1 => train (x_, y_)
        case 2 => train2 (x_, y_)
        case _ => flaw ("trainSwitch", s"which = $which not in (0, 1, 2)"); null
        end match
    end trainSwitch
     */

    //::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
    /** Given a new input vector 'z', predict the output/response value 'f(z)'.
     *  @param z  the new input vector
     */
    override def predict (z: VectorD): Double = f.f (b dot z)

    //::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
    /** Given a new input matrix 'z', predict the output/response value 'f(z)'.
     *  @param z  the new input matrix
     */
    override def predict (z: MatrixD = x): VectorD = f.f_ (z * b)

    //::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
    /** Build a sub-model that is restricted to the given columns of the data matrix.
     *  @param x_cols  the columns that the new model is restricted to
     */
    override def buildModel (x_cols: MatrixD): Perceptron =
        new Perceptron (x_cols, y, null, hparam, f, itran)
    end buildModel

end Perceptron


object Perceptron:

    /** hyper-parameters for tuning the optimization algorithms - user tuning
     */
    val hp = new HyperParameter
    hp += ("eta", 0.1, 0.1)                                              // learning/convergence rate
    hp += ("bSize", 20, 20)                                              // mini-batch size, common range 10 to 30
    hp += ("maxEpochs", 500, 500)                                        // maximum number of epochs/iterations

end Perceptron

