
//::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
/** @author  Dong Yu Yu, John Miller
 *  @version 2.0
 *  @date    Wed Nov  7 17:08:17 EST 2018
 *  @see     LICENSE (MIT style license file).
 *
 *  @title   Model: Regression Tree
 */

package scalation
package modeling

import scala.collection.mutable.{ArrayBuffer, Queue, Set}
import scala.runtime.ScalaRunTime.stringOf

import scalation.mathstat._
import scalation.random.PermutedVecI

// FIX - rSqBar from validate is wrong

//::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
/** The `RegressionTree` companion object is used to count the number of leaves
 *  and provide factory functions.
 */
object RegressionTree:

    val hp = new HyperParameter                                       // default hyper-parameter values
    hp += ("maxDepth", 5, 5)
    hp += ("threshold", 0.1, 0.1)

    private var nLeaves_ = 0                                          // the number of leaves in the tree

    //::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
    /** Return the number of leaves in the tree.
     */
    def nLeaves: Int = nLeaves_

    //::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
    /** Icrement the number of leaves in the tree.
     */
    def incLeaves (): Unit = nLeaves_ += 1

    //::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
    /** Reset the number of leaves in the tree.
     */
    def resetLeaves (): Unit = nLeaves_ = 0

    //::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
    /** Create a `RegressionTree` object from a combined data-response matrix.
     *  @param xy      the combined data-response matrix
     *  @param fname   the names for all features/variables
     *  @param hparam  the hyper-parameters
     *  @param col     the designated response column (defaults to the last column)
     */
    def apply (xy: MatrixD, fname: Array [String] = null,
               hparam: HyperParameter = hp)(col: Int = xy.dim2 - 1): RegressionTree =
        new RegressionTree (xy.not(?, col), xy(?, col), fname, hparam)
    end apply

    //::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
    /** Create a `RegressionTree` object from a data matrix and response vector.
     *  @param x       the data matrix
     *  @param y       the response vector
     *  @param fname   the names for all features/variables
     *  @param hparam  the hyper-parameters
     */
    def rescale (x: MatrixD, y: VectorD, fname: Array [String] = null,
               hparam: HyperParameter = hp): RegressionTree =
        val xn = normalize ((x.mean, x.stdev)) (x)
        new RegressionTree (xn, y, fname, hparam)
    end rescale

end RegressionTree

import RegressionTree.{nLeaves, incLeaves, resetLeaves}

//::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
/** The `Node` class contains information for a tree node.
 *  @param f        the feature of the node used for splitting, if it is leaf, contains the feature of its parent
 *  @param branch   the branch value (0 => left, 1 => right)
 *  @param yp       leaf node's prediction for y
 *  @param thresh   the threshold for continuous feature
 *  @param depth    the current depth of the node
 *  @param pthresh  the threshold for parent node
 *  @param pfea     the feature of parent node
 *  @param leaf     `Boolean` value indicate whether is a leaf node
 */
case class Node (f: Int, branch: Int, yp: Double, thresh: Double,
                 depth: Int, pthresh: Double, pfea: Int, leaf: Boolean = false):

    val child = new ArrayBuffer [Node] ()                             // children of node

    //::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
    /** Convert node to a string.
     */
    override def toString: String =
        if child.length == 0 then
            s"Leaf (pfeature = x$pfea, branch = $branch, feature = x$f, yp = $yp)"
        else if depth == 0 then
            s"Root (feature = x$f, threshold = $thresh)"
        else
            s"Node (pfeature = x$pfea, branch = $branch, feature = x$f, threshold = $thresh)"
    end toString

end Node


//::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
/** The `RegressionTree` class implements a Regression Tree that selects splitting features
 *  using minimal variance in children nodes.  To avoid exponential choices in the selection,
 *  supporting ordinal features currently.
 *  @param x            the m-by-n input/data matrix
 *  @param y            the response m-vector
 *  @param fname_       the names of the model's features/variables
 *  @param hparam       the hyper-parameters for the model
 *  @param curDepth     current depth
 *  @param branchValue  the branch value for the tree node
 *  @param feature      the feature for the tree's parent node
 */
class RegressionTree (x: MatrixD, y: VectorD, fname_ : Array [String] = null,
                      hparam: HyperParameter = RegressionTree.hp,
                      curDepth: Int = -1, branchValue: Int = -1, feature: Int = -1)
      extends Predictor (x, y, fname_, hparam)
         with Fit (dfm = x.dim2 - 1, df = x.dim - x.dim2):            // call resetDF once tree is built

    private val debug     = debugf ("RegressionTree", true)           // debug function
    private val (m, n)    = (x.dim, x.dim2)                           // matrix dimensions
    private val maxDepth  = hparam ("maxDepth").toInt                 // the depth limit for tree
    private val thres     = hparam ("threshold").toDouble             // the threshold for the tree's parent node
    private val threshold = new Array [(Double, Double)] (n)          // store best splitting threshold for each feature
    private val stream    = 0                                         // the random number stream
    private val permGen   = PermutedVecI (VectorI.range (0, m), stream)
    private var root: Node = null                                     // root node   

    modelName = "RegressionTree"

    debug ("init", s"Constructing a Regression Tree: curDepth = $curDepth")

    //::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
    /** Split gives row indices of left and right children when splitting using thresh.
     *  @param j       the column/feature to use
     *  @param thresh  the threshold for splitting (below => left, above => right) 
     */
    private def split (j: Int, thresh: Double): (IndexedSeq [Int], IndexedSeq [Int]) =
        val (sLeft, sRight) = (Set [Int] (), Set [Int] ())
        for i <- x.indices do if x(i, j) <= thresh then sLeft += i else sRight += i
        (sLeft.toIndexedSeq, sRight.toIndexedSeq)
    end split

    //::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
    /** Given feature f, use fast threshold selection to find an optimal threshold/
     *  split point in O(NlogN) time.
     *  @see people.cs.umass.edu/~domke/courses/sml/12trees.pdf
     *  @param f          the given feature for which the threshold is desired
     *  @param x_f        column f in data matrix
     *  @param subSample  optional, use to select from the range
     */
    def fastThreshold (f: Int, x_f: VectorD, subSample: VectorI = null): Unit =
        var thres = 0.0                                               // to hold optimal threshold
        var tSSE  = Double.MaxValue                                   // total sum of squared errors
        var ref   = Array.ofDim [(Double, Int)] (y.dim)               // pair column value with column index
        for i <- x.indices do ref(i) = (x_f(i), i)                    // assign pairs
        ref = ref.sortBy (_._1)                                       // sort by column value

        val values = x_f.distinct.sorted                              // get distinct values from column x_f & sort

        val v = new VectorD (values.dim - 1)                          // mid points between all values
        if v.dim == 0 then { threshold(f) = (thres, -1.0); return }   // no values => return early
        for i <- v.indices do v(i) = (values(i) + values(i+1)) / 2.0

        val (totalSum, totalSqr) = (y.sum, y.normSq)                  // total sum and sum of squares
        var sum, square, mean = 0.0                                   // left sum, square and mean
        var (row, valu) = (0, v(0))                                   // candidate split value/threshold

        for i <- ref.indices do
            if ref(i)._1 > valu then
                val n_i   = ref.size - i                              // number of elements on left
                val rSum  = totalSum - sum                            // right sum
                val rSqr  = totalSqr - square                         // right sum of squares
                val rMean = rSum / n_i                                // right mean
                val lrSSE = square - 2 * sum  * mean  + i   * mean  * mean +
                            rSqr   - 2 * rSum * rMean + n_i * rMean * rMean

                if lrSSE < tSSE then { tSSE = lrSSE; thres = valu }   // update if lrSSE is smaller
                row += 1
            end if

            val yi  = y(ref(i)._2)
            sum    += yi                                              // left sum
            square += yi * yi                                         // left sum of squares
            mean    = (yi + i * mean) / (i + 1)                       // left mean
            if row < v.dim then valu = v(row)
        end for

        threshold(f) = (thres, tSSE)                                  // return best split point
    end fastThreshold

    //::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
    /** Return new x matrix and y vector for next step of constructing regression tree.
     *  @param f     the feature index
     *  @param side  indicator for which side of child is chosen (i.e., 0 for left child)
     */
    def nextXY (f: Int, side: Int): (MatrixD, VectorD) =
        val (left, right) = split (f, threshold(f)._1)
        if side == 0 then (x(left), y(left)) else (x(right), y(right))
    end nextXY

    //::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
    /** Train the regression tree by selecting threshold for all the features
     *  in y_ (can be used as all the samples or sub-samples).
     *  @param x_  the training/full data/input matrix
     *  @param y_  the training/full response/output vector
     *                 only the values in y_ will be used in selecting threshold
     */
    def train (x_ : MatrixD, y_ : VectorD): Unit =
        // FIX - other methods need x_ , currently only works for x_  = x
        train (VectorI.range (0, y_.dim))
    end train

    //::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
    /** Train the regression tree by selecting threshold for all the features
     *  in interval (sub-samples).
     *  @param interval  only the values in interval will be used in selecting threshold
     */
    def train (interval: VectorI): RegressionTree =
        for f <- 0 until n do fastThreshold (f, x(?, f), interval)    // set threshold for features
        var opt = (0, threshold(0)._2)                                // compute variance for feature 0

        debug ("train", s"for feature ${opt._1} the variance is ${opt._2}")

        for f <- 1 until n do
            val fVar = threshold(f)._2
            debug ("train", s"for feature $f the variance is $fVar")
            if fVar <= opt._2 then opt = (f, fVar)                    // save feature giving minimal variance
        end for

        debug ("train", s"optimal feature is ${opt._1} with variance of ${opt._2}")
        buildTree (opt)
        this
    end train

    //::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
    /** Given the next most distinguishing feature/attribute, extend the regression tree.
     *  @param opt  the optimal feature and the variance
     */
    def buildTree (opt: (Int, Double)): Unit =
        root =
            if curDepth == 0 then Node (opt._1, -1, y.mean, threshold(opt._1)._1, curDepth, -1.0, -1)
            else Node (opt._1, branchValue, y.mean, threshold(opt._1)._1, curDepth, thres, feature)
        debug ("buildTree", s"--> Add root = ${root}")

        for i <- 0 until 2 do                                         // 0 => left, 1 => right
            val next = nextXY (opt._1, i)
            if next._2.size != 0 then
                root.child +=
                    (if curDepth == maxDepth - 1 || next._2.size <= x.dim2 then
                        val yp = next._2.mean
                        incLeaves ()
                        Node (opt._1, root.child.length, yp, threshold(opt._1)._1, curDepth + 1,
                              threshold(opt._1)._1, opt._1, true)
                    else
                        val hp = RegressionTree.hp.updateReturn ("threshold", threshold(opt._1)._1)
                        val subtree = new RegressionTree (next._1, next._2, fname, hp, curDepth + 1, i, opt._1)
                        subtree.train (next._1, next._2)
                        subtree.root)

                debug ("buildTree", s"--> Add child = ${root.child}")
//              debug ("buildTree", s"\t x \t = ${next._1} \n\t y \t = ${next._2}")
            end if
        end for
    end buildTree

    //::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
    /** Test a predictive model y_ = f(x_) + e and return its QoF vector.
     *  Testing may be be in-sample (on the training set) or out-of-sample
     *  (on the testing set) as determined by the parameters passed in.
     *  Note: must call train before test.
     *  @param x_  the testing/full data/input matrix (defaults to full x)
     *  @param y_  the testing/full response/output vector (defaults to full y)
     */
    def test (x_ : MatrixD = x, y_ : VectorD = y): (VectorD, VectorD) =
        val yp = predict (x_)                                         // make predictions
        val df1 = nLeaves                                             // degrees of freedom model = number of leaves
        val df2 = y_.dim - df1                                        // degrees of freedom error
        resetDF ((df1, df2))
        (yp, diagnose (y_, yp))                                       // return predictions and QoF vector
    end test

    //::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
    /** Print the regression tree in Pre-Order using printT method. 
     */
    def printTree (): Unit =
        println ("Regression Tree: nLeaves = " + nLeaves)
        println ("fname = " + stringOf (fname))
        printT (root, 0)
        println ()
    end printTree

    //::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
    /*  Recursively print the regression tree nodes.
     *  @param nod     the current node
     *  @param level   the level of node nod in the tree
     */
    def printT (nod: Node, level: Int): Unit =
        println ("\t" * level + "[ " + nod + " ]")
        for cnode <-nod.child do printT (cnode, level + 1)
    end printT

    //::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
    /** Print out the regression tree using Breadth First Search (BFS).
     */
    def printTree2 (): Unit =
        println ("RegressionTree:")
        println ("fname = " + stringOf (fname))
        val queue = new Queue [Node] ()

        for cnode <- root.child do queue += cnode
        println (root)
        var level = 0

        while ! queue.isEmpty do
            val size = queue.size
            level   += 1
            for i <- 0 until size do
                val nod = queue.dequeue ()
                println ("\t" * level + "[ " + nod + " ]")
                for cnode <- nod.child do queue += cnode
            end for
            println ()
        end while
    end printTree2

    //::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
    /** Given a data vector z, predict the value by following the tree to the leaf.
     *  @param z  the data vector to predict
     */
    override def predict (z: VectorD): Double =
        var nd = root                                                 // current node
        while nd.child.length >= 2 do
            nd = if z(nd.f) <= nd.thresh then nd.child(0) else nd.child(1)
        end while
        nd.yp
    end predict

    //::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
    /** Given a data matrix z, predict the value by following the tree to the leaf.
     *  @param z  the data matrix to predict
     */
    override def predict (z: MatrixD = x): VectorD =
        VectorD (for i <- z.indices yield predict (z(i)))
    end predict

    //::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
    /** Build a sub-model that is restricted to the given columns of the data matrix.
     *  @param x_cols  the columns that the new model is restricted to
     */
    override def buildModel (x_cols: MatrixD): RegressionTree =
        new RegressionTree (x_cols, y, null, hparam)
    end buildModel

    //::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
    /** Reset or re-initialize the frequency tables and the probability tables.
     */
    def reset (): Unit = resetLeaves ()

end RegressionTree


//::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
/** The `regressionTreeTest` main function is used to test the `RegressionTree` class.
  *  It tests a simple case that does not require a file to be read.
  *  @see translate.google.com/translate?hl=en&sl=zh-CN&u=https:
  *       //www.hrwhisper.me/machine-learning-decision-tree/&prev=search
  *  > runMain scalation.modeling.regressionTreeTest
  */
@main def regressionTreeTest (): Unit =

    val x  = MatrixD ((10, 1), 1, 2, 3, 4, 5, 6, 7, 8, 9, 10)
    val y  = VectorD (5.56, 5.70, 5.91, 6.40, 6.80, 7.05, 8.90, 8.70, 9.00, 9.05)
    val ox = VectorD.one (x.dim) +^: x
    val fname = Array ("x")

    banner (s"Regression no intercept")
    val reg = new Regression (x, y)
    reg.trainNtest ()()                                               // train and test the model

    banner (s"Regression with intercept")
    val reg2 = new Regression (ox, y)
    reg2.trainNtest ()()                                              // train and test the model

    banner (s"Quadratic Regression")
    val reg3 = SymbolicRegression.quadratic (x, y, fname)
    reg3.trainNtest ()()                                              // train and test the model

    banner (s"Perceptron sigmoid")
    val nn = Perceptron.rescale (reg3.getX, y)
    nn.trainNtest ()()                                                // train and test the model

    banner (s"Perceptron tanh")
    val nn2 = Perceptron.rescale (reg3.getX, y, f = ActivationFun.f_tanh)
    nn2.trainNtest ()()                                               // train and test the model

    for d <- 0 to 4 do
        banner (s"Regression Tree with maxDepth = $d")
        val hp2 = RegressionTree.hp.updateReturn ("maxDepth", d)
        val mod = new RegressionTree (x, y, null, hp2)
        mod.trainNtest ()()                                           // train and test the model
        mod.printTree ()
        mod.reset ()
    end for

end regressionTreeTest


//::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
/** The `regressionTreeTest2` main function tests the `RegressionTree` class using the AutoMPG
 *  dataset.  Assumes no missing values.  It tests forward, backward and stepwise selection.
 *  > runMain scalation.modeling.regressionTreeTest2
 */
@main def regressionTreeTest2 (): Unit =

    import Example_AutoMPG._

//  println (s"x = $o")
//  println (s"y = $y")

    banner ("auto_mpg Regression Tree")
    val mod = new RegressionTree (x, y, x_fname)                     // create model with intercept (else pass x)
    mod.trainNtest ()()                                              // train and test the model
    mod.printTree ()                                                 // print the regression tree
//  println (mod.summary ())                                         // parameter/coefficient statistics

/*
    banner ("Cross-Validation")
    Fit.showQofStatTable (mod.crossValidate ())

    println (s"ox_fname = ${stringOf (ox_fname)}")

    for tech <- Predictor.SelectionTech.values do
        banner (s"Feature Selection Technique: $tech")
        val (cols, rSq) = mod.selectFeatures (tech)                   // R^2, R^2 bar, R^2 cv
        val k = cols.size
        println (s"k = $k, n = ${x.dim2}")
        new PlotM (null, rSq.transpose, Array ("R^2", "R^2 bar", "R^2 cv"),
                   s"R^2 vs n for Regression Tree with $tech", lines = true)
        println (s"$tech: rSq = $rSq")
    end for
*/
    banner ("Quasi Feature Selection via max-depth constraint")
    for d <- 1 to 6 do
        banner (s"Regression Tree with maxDepth = $d")
        val hp2 = RegressionTree.hp.updateReturn ("maxDepth", d)
        val mod = new RegressionTree (x, y, x_fname, hp2)
        mod.trainNtest ()()                                           // train and test the model
        mod.printTree ()
        mod.reset ()
    end for

end regressionTreeTest2

