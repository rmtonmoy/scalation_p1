//:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
/** @author Yulong
  * @version 1.0
  * @date Thursday Feb 17 13:32:52 EDT 2022
  * @see LICENSE (MIT style license file).
  * @title Simultaneous perturbation stochastic approximation
  */

package scalation
package optimization

import scala.math.pow
import scalation.mathstat._
import scalation.random.Bernoulli


//:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

//link1: https://www.jhuapl.edu/spsa/PDF-SPSA/Matlab-SPSA_Alg.pdf

class SPSA (f: FunctionV2S, theta: VectorD, nSteps: Int = 20):
//    extends Minimizer:

    private val debug = debugf ("SPSA", true)                       // debug function
    //private var best  = (f(x), x)                                 // location zero solution

    private val alpha  = 0.602
    private val gamma  = 0.101
    private val A      = 100
    private val a      = 0.16              // these numbers are from Spall (1998) DOI: 10.1109/7.705889
    private val c      = 1
    private val berny  = Bernoulli (0.5,5)
    private var best   = theta             // best theta or parameter to get the lowest error from loss function

    //:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
    /**
     */
    def basic (): (Double, VectorD) =
        for k <- 0 to nSteps do
            val ak       = a / pow(A+k+1, alpha)
            val ck       = c / pow(k+1, gamma)
            val ck_delta = ck * (2 * berny.igen - 1)            // ck * (-1 or 1)
            val yplus    = f(best + ck_delta)
            val yminus   = f(best - ck_delta)
            val ghat     = (yplus - yminus) / (2 * ck_delta)
            best        -= ak * ghat
        end for
        debug ("basic", s"best = $best")
        (f(best), best)
    end basic

    //:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
    /**
     */
    def fastconvergence2 (): (Double, VectorD) =
        // Reject iteration k  → k + 1 if ||θ_k+1 - θ_k ||  is too
        // large (does not require extra loss measurement)
        // theta_k = theta_k+1 ??

        // needs to define what is large ??
        var better = theta
        val large  = 0.11

        for k <- 1 to nSteps do
            val ak       = a / pow(A+k+1, alpha)
            val ck       = c / pow(k+1, gamma)
            val ck_delta = ck * (2 * berny.igen - 1)            // shift = ck * (-1 or 1)
            val yplus    = f(best + ck_delta)                   // function at + shift
            val yminus   = f(best - ck_delta)                   // function at - shift
            val ghat     = (yplus - yminus) / (2 * ck_delta)    // slope
            better       = best                                 // save previous best
            best        -= ak * ghat                            // move opposite slope

            val reject = best - better
            if reject.norm > large then
                println (s"No $k $better")
                debug ("fastconvergence2", s"k = $k, best = $best")
                return (f(better), better)
            end if
        end for
        debug ("fastconvergence2", s"k = $nSteps, best = $best")
        (f(best), best)
    end fastconvergence2

    //other fast convergence from link1 to be looked into later.

end SPSA


//:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
/** The sPSATest main function if used to test the `SPSA` class.
 *  > runMain scalation.optimization.sPSATest
 */
@main def sPSATest (): Unit =

    banner ("Minimize: (x_0 - 3)^2 + (x_1 - 4)^2 + 1")
    def f (x: VectorD): Double = (x(0) - 3.0)~^2 + (x(1) - 4.0)~^2 + 1.0
    val x0 = VectorD (1, 2)
    val optimizer = new SPSA (f, x0, 200)
    val opt = optimizer.basic ()
    println (s"][ optimal solution (f(x), x) = $opt")

end sPSATest

//:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
/** The sPSATest2 main function if used to test the `SPSA` class.
 *  > runMain scalation.optimization.sPSATest2
 */
@main def sPSATest2 (): Unit =

    banner ("Minimize: (x_0 - 3)^2 + (x_1 - 4)^2 + 1")
    def f (x: VectorD): Double = (x(0) - 3.0)~^2 + (x(1) - 4.0)~^2 + 1.0
    val x0 = VectorD (1, 2)
    val optimizer = new SPSA (f, x0, 200)
    val opt = optimizer.fastconvergence2 ()
    println (s"][ optimal solution (f(x), x) = $opt")

end sPSATest2

